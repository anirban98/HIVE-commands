CREATE SCHEMA/DATABASE IF NOT EXISTS userdb;

CREATE SCHEMA userdb;

SHOW DATABASES;

DROP SCHEMA/DATABASE IF EXISTS userdb CASCADE;

=========================================================================================
						    Managed/Internal Table
=========================================================================================
Need To Create File in Local File System : Employee.txt

1201	Gopal	45000	Technical manager
1202	Manisha	45000	Proof reader 
1203	Masthanvali	40000	Technical writer 
1204	Krian	40000	Hr Admin
1205	Kranthi	30000	Op Admin


CREATE TABLE employee ( eid int, ename String, esalary int, edesignation String) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'; 

LOAD DATA local INPATH '/home/training/Employee.txt' 
     INTO TABLE employee;
 
LOAD DATA INPATH '/user/training/Employee' 
     INTO TABLE employee;

Select * from employee;
	 
set hive.cli.print.header = true;

Select * from employee;

========================================================================================
			              External Table Without Location
=========================================================================================
Need To Create File in Local File System : Cricket.txt

Sachin	Ind	100
Dravid	Ind	60
Ponting	Aus	68
Kallis	SA	70
Tylor	NZ	25
Cook	Eng	30

CREATE EXTERNAL TABLE Cricket ( Name String, Country String, Run int ) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'; 

LOAD DATA local INPATH '/home/training/Cricket.txt' 
     INTO TABLE Cricket;    

=========================================================================================
					       External Table With Location  
=========================================================================================
Need To Create File in Local File System : Cricket.txt

Sachin	Ind	100
Dravid	Ind	60
Ponting	Aus	68
Kallis	SA	70
Tylor	NZ	25
Cook	Eng	30

CREATE TABLE IF NOT EXISTS Cricket ( NAME String, Country String, Run int )
    ROW FORMAT DELIMITED
    FIELDS TERMINATED BY '\t' 
    location '/user/training/Cricket/'
    
LOAD DATA local INPATH '/home/training/Cricket.txt' 
     INTO TABLE Cricket;

LOAD DATA INPATH '/user/training/cr' 
     INTO TABLE Cricket;

=========================================================================================
						        Alter Table Statement 
=========================================================================================

ALTER TABLE name RENAME TO new_name
ALTER TABLE name ADD COLUMNS (col_spec[, col_spec ...])
ALTER TABLE name DROP [COLUMN] 
column_name
ALTER TABLE name CHANGE column_name new_name new_type
ALTER TABLE name REPLACE COLUMNS (col_spec[, col_spec ...])


ALTER TABLE employee RENAME TO emp;

ALTER TABLE employee CHANGE name ename String;

ALTER TABLE employee CHANGE salary salary Double;

ALTER TABLE employee ADD COLUMNS ( dept STRING COMMENT 'Department name');

ALTER TABLE employee REPLACE COLUMNS ( eid INT  empid Int, ename STRING name String );

=========================================================================================
			Drop a Table : ( Difference between External and Internal Table )
=========================================================================================


 
=========================================================================================
								 Complex Data Type in Hive
=========================================================================================
ARRAY
=====================

cat > arrayfile

1,abc,40000,a$b$c,hyd
2,def,3000,d$f,bang

hive> create table Emoloye_Array (id int,name string,sal bigint,sub array<string>,city string) row format delimited  fields terminated by ',' collection items terminated by '$';

load data local inpath '/home/training/arrayfile' 
into table Emoloye_Array;

hive> select sub[2] from Emoloye_Array where id=1;

hive> select sub[0] from Emoloye_Array;

======================
MAP:
======================

cat > mapfile
1,abc,40000,a$b$c,pf#500$epf#200,hyd
2,def,3000,d$f,pf#500,bang

hive> create table Emoloye_Array_Map(id int,name string,sal bigint,sub array<string>,dud map<string,int>,city string)
row format delimited
fields terminated by ','
collection items terminated by '$'
map keys terminated by '#';

hive> load data local inpath ‘/home/training/mapfile’ overwrite into table Emoloye_Array_Map;

hive> select dud["pf"] from Emoloye_Array_Map;

hive> select dud["pf"],dud["epf"] from Emoloye_Array_Map;

=======================
STRUCT:
=======================

cat > mapfile
1,abc,40000,a$b$c,pf#500$epf#200,hyd$ap$500001
2,def,3000,d$f,pf#500,bang$kar$600038

hive> create table Emoloye_Array_Map_Stuc (id int,name string,sal bigint,sub array<string>,dud map<string,int>,addr struct<city:string,state:string,pin:bigint>)
row format delimited
fields terminated by ','
collection items terminated by '$'
map keys terminated by '#';

hive> load data local inpath '/home/training/structfile' into table Emoloye_Array_Map_Stuc;

hive> select addr.city from Emoloye_Array_Map_Stuc;

=========================================================================================
						   Static Partitios in Hive
=========================================================================================

Actual Data

1201	Gopal	45000	Technical Manager	Kolkata
1202	Manisha	45000	ProofReader	Kolkata
1203	Masthanvali	40000	TechnicalWriter	Delhi
1204	Krian	40000	Hr Admin	Delhi
1205	Kranthi	30000	Op Admin	Mumbai
1206	Avishek	50000	Developer	Mumbai





Need To Create File As Kolkata.txt

1201	Gopal	45000	Technical Manager
1202	Manisha	45000	Proof Reader	

Need To Create File As Delhi.txt

1203	Masthanvali	40000	Technical Writer
1204	Krian	40000	Hr Admin

Need To Create File As Mumbai.txt

1205	Kranthi	30000	Op Admin 
1206	Avishek	50000	Developer

CREATE TABLE employee ( eid int, ename String, esalary int, edesignation String )
    partitioned by ( loc String ) 
    ROW FORMAT DELIMITED
    FIELDS TERMINATED BY '\t';
	 
LOAD DATA LOCAL INPATH '/home/training/Kolkata.txt' 
     INTO TABLE employee
     partition ( loc = 'Kolkata');

LOAD DATA LOCAL INPATH '/home/training/Delhi.txt' 
     INTO TABLE employee
     partition ( loc = 'Delhi');

LOAD DATA LOCAL INPATH '/home/training/Mumbai.txt' 
     INTO TABLE employee
     partition ( loc = 'Mumbai');

=========================================================================================
						   Dynamic Partitios in Hive
=========================================================================================

create table txnrecords(txnno INT, txndate STRING, custno INT, amount DOUBLE, 
category STRING, product STRING, city STRING, state STRING, spendby STRING)
row format delimited
fields terminated by ','
stored as textfile;

LOAD DATA LOCAL INPATH '/home/training/txns' OVERWRITE INTO TABLE txnrecords;

create table txnrecsByCat(txnno INT, txndate STRING, custno INT, amount DOUBLE,
product STRING, city STRING, state STRING, spendby STRING)
partitioned by (category STRING)
row format delimited
fields terminated by ','
stored as textfile;

set hive.exec.dynamic.partition.mode=nonstrict;
set hive.exec.dynamic.partition=true;

from txnrecords txn INSERT OVERWRITE TABLE txnrecsByCat PARTITION(category)
select txn.txnno, txn.txndate,txn.custno, txn.amount,txn.product,txn.city,txn.state,
txn.spendby, txn.category DISTRIBUTE BY category;

=========================================================================================
					Import Data from one table to another table
========================================================================================
Need To Create File As source.txt

1201     Gopal          45000     Technical manager
1202     Manisha        45000     Proof reader 
1203     Masthanvali    40000     Technical writer 
1204     Krian          40000     Hr Admin
1205     Kranthi        30000     Op Admin 

hadoop fs -put source.txt source

CREATE TABLE Source( eid int, ename String, esalary String, edesignation String)  ROW FORMAT DELIMITED    FIELDS TERMINATED BY '\t' ;

LOAD DATA LOCAL INPATH '/home/training/source.txt' 
     INTO TABLE Source;
 
LOAD DATA INPATH '/user/training/source' 
     INTO TABLE Source;

Select * from source;

CREATE TABLE Target ( id int, name String, salary String, desigation String)
    ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';

insert overwrite table Target select eid,ename,esalary,edesignation from Source;

insert table Target as select * from Source;

=========================================================================================
								Relation Operator :
=========================================================================================
Need To Create File As Empl.txt

7369,SMITH,CLERK,7902,800,,20
7499,ALLEN,SALESMAN,7698,1600,300,30
7521,WARD,SALESMAN,7698,1250,500,30
7566,JONES,MANAGER,7839,2975,,20
7654,MARTIN,SALESMAN,7698,1250,1400,30
7698,BLAKE,MANAGER,7839,2850,,30
7782,CLARK,MANAGER,7839,2450,,10
7788,SCOTT,ANALYST,7566,3000,,20
7839,KING,PRESIDENT,,5000,,10
7844,TURNER,SALESMAN,7698,1500,0,30
7876,ADAMS,CLERK,7788,1100,,20
7900,JAMES,CLERK,7698,950,,30
7902,FORD,ANALYST,7566,3000,,20
7934,MILLER,CLERK,7782,1300,,10


CREATE TABLE employee( EMPNO int, ENAME String, JOB String, MGR int, SAL int, COMM int, DEPTNO int ) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';

Load data local inpath '/home/training/empl.txt'
	into table employee;

SELECT * FROM employee WHERE Id=1205;

SELECT * FROM employee WHERE Salary>=40000;

=========================================================================================
								Arithmetic Operator
=========================================================================================
SELECT 20+30 ADD FROM employee;

=========================================================================================
								Logical Operator
=========================================================================================
SELECT * FROM employee WHERE Salary>40000 && Dept=TP;


=========================================================================================
								Built In Functions
=========================================================================================
SELECT round(2.6) from employee;
SELECT floor(2.6) from employee;
SELECT ceil(2.6) from employee;

=========================================================================================
								Aggregate Funtions 
=========================================================================================
Avg , count , max , min,..

=========================================================================================
						Where cluase , order by cluase , group by cluase::
=========================================================================================
SELECT * FROM employee WHERE salary>30000;

SELECT Id,Name, Dept FROM employee ORDER BY DEPT desc;

SELECT Dept,title,count(*) FROM employee GROUP BY DEPT,title;

=========================================================================================
						       Hive Join:::
=========================================================================================
Inner Join  ( Eui-Join, non eqi-join , self join )

Equi Join: http://www.w3resource.com/sql/joins/perform-an-equi-join.php



Need To Create File As Empl.txt

7369,SMITH,CLERK,7902,800,,20
7499,ALLEN,SALESMAN,7698,1600,300,30
7521,WARD,SALESMAN,7698,1250,500,30
7566,JONES,MANAGER,7839,2975,,20
7654,MARTIN,SALESMAN,7698,1250,1400,30
7698,BLAKE,MANAGER,7839,2850,,30
7782,CLARK,MANAGER,7839,2450,,10
7788,SCOTT,ANALYST,7566,3000,,20
7839,KING,PRESIDENT,,5000,,10
7844,TURNER,SALESMAN,7698,1500,0,30
7876,ADAMS,CLERK,7788,1100,,20
7900,JAMES,CLERK,7698,950,,50
7902,FORD,ANALYST,7566,3000,,60
7934,MILLER,CLERK,7782,1300,,50


EMPNO NUMBER(4),
ENAME VARCHAR2(10),
JOB VARCHAR2(9),
MGR NUMBER(4),
SAL NUMBER(7,2),
COMM NUMBER(7,2),
DEPTNO NUMBER(2)

CREATE TABLE IF NOT EXISTS Empl( EMPNO int, ENAME String, JOB String, MGR int, SAL int, COMM int, DEPTNO int )
 -- COMMENT 'Employee details'
    ROW FORMAT DELIMITED
    FIELDS TERMINATED BY ',' LINES TERMINATED BY '\n'
 --STORED AS TEXTFILE;
 
  LOAD DATA LOCAL INPATH '/home/training/Empl.txt' INTO table Empl;

Need To Create File As Dept.txt

10,ACCOUNTING,NEY WORK
20,RESEARCH,DALLAS
30,SALES,CHICAGO
40,OPERATIONS,BOSTON

DEPTNO DNAME LOC

CREATE TABLE IF NOT EXISTS Dept ( DEPTNO int, DNAME String, LOC String )
 -- COMMENT 'Employee details'
    ROW FORMAT DELIMITED
    FIELDS TERMINATED BY ',' LINES TERMINATED BY '\n'
 --STORED AS TEXTFILE;

  LOAD DATA LOCAL INPATH '/home/training/Dept.txt' INTO table Dept;

 
Inner Join :
=========================================================================================
select s.*, d.* from Empl s JOIN Dept d ON ( s.deptno = d.deptno );

Example:
hive> SELECT * FROM sales;
Joe 2
Hank 4
Ali 0
Eve 3
Hank 2

hive> SELECT * FROM things;
2 Tie
4 Coat
3 Hat
1 Scarf

We can perform an inner join on the two tables as follows:

hive> SELECT sales.*, things.*
> FROM sales JOIN things ON (sales.id = things.id);

Joe 2 2 Tie
Hank 4 4 Coat
Eve 3 3 Hat
Hank 2 2 Tie


Outer Join :
=========================================================================================
select empl.*, dept.* from empl left outer join dept on ( empl.deptno = dept.deptno );

select empl.*, dept.* from empl right outer join dept on ( empl.deptno = dept.deptno );

select empl.*, dept.* from empl full outer join dept on ( empl.deptno = dept.deptno );

Example:
Outer joins allow you to find nonmatches in the tables being joined. In the current
example, when we performed an inner join, the row for Ali did not appear in the output,
because the ID of the item she purchased was not present in the things table. If we
change the join type to LEFT OUTER JOIN, the query will return a row for every row in
the left table (sales), even if there is no corresponding row in the table it is being joined
to (things):

hive> SELECT sales.*, things.*
> FROM sales LEFT OUTER JOIN things ON (sales.id = things.id);

Joe 2 2 Tie
Hank 4 4 Coat
Ali 0 NULL NULL
Eve 3 3 Hat
Hank 2 2 Tie

Notice that the row for Ali is now returned, and the columns from the things table are
NULL because there is no match.

Hive also supports right outer joins, which reverses the roles of the tables relative to the
left join. In this case, all items from the things table are included, even those that weren’t
purchased by anyone (a scarf):

hive> SELECT sales.*, things.*
> FROM sales RIGHT OUTER JOIN things ON (sales.id = things.id);

Joe 2 2 Tie
Hank 2 2 Tie
Hank 4 4 Coat
Eve 3 3 Hat
NULL NULL 1 Scarf

Finally, there is a full outer join, where the output has a row for each row from both
tables in the join:

hive> SELECT sales.*, things.*
> FROM sales FULL OUTER JOIN things ON (sales.id = things.id);

Ali 0 NULL NULL
NULL NULL 1 Scarf
Hank 2 2 Tie
Joe 2 2 Tie
Eve 3 3 Hat
Hank 4 4 Coat

=========================================================================================
						Hive Doesnot support IN Subquires
========================================================================================
Select * from empl where empl.id IN ( Select id from dept );

select *  from empl left semi join dept on ( empl.deptno = dept.deptno );

=========================================================================================
								 UDF in Hive
=========================================================================================
Create table Fruits.txt

1	Mango	100
2	 Apple	200
3	  Grape	300
4	Banana	50
5	Guava	20


CREATE TABLE IF NOT EXISTS Fruits ( id int, name String, price int )
    ROW FORMAT DELIMITED
    FIELDS TERMINATED BY '\t'; 
 
LOAD DATA local INPATH '/home/training/Fruits.txt' 
     OVERWRITE INTO TABLE Fruits;
	 
import org.apache.commons.lang.StringUtils;
import org.apache.hadoop.hive.ql.exec.UDF;
import org.apache.hadoop.io.Text;

public class TestUDF extends UDF {
	private Text result = new Text();

	public Text evaluate(Text str) {
		if (str == null) {
			return null;
		}
		result.set(StringUtils.strip(str.toString()));
		return result;
	}

	public Text evaluate(Text str, String stripChars) {
		if (str == null) {
			return null;
		}
		result.set(StringUtils.strip(str.toString(), stripChars));
		return result;
	}
}

select name from fruits;	 
	 
Hive > add jar /home/training/hiveudf.jar;

Hive > create temporary function tr as 'TestUDF';

select id,tr( name),price from fruits;
 
=========================================================================================
Create a file name student_record.txt

101	Tom	9	358.0	85.0	95.0	86.0	92.0	2015
102	Jerry	9	360.0	90.0	90.0	90.0	90.0	2015
103	Harry	9	370.0	95.0	95.0	85.0	95.0	2015
104	James	9	350.0	84.0	90.0	86.0	90.0	2015
105	Rohit	9	362.0	92.0	98.0	82.0	90.0	2015

CREATE TABLE Student ( id int, name String, class int, total_marks double,math double,english double,physis double,chemistry double,year int)
    ROW FORMAT DELIMITED
    FIELDS TERMINATED BY '\t';
 
LOAD DATA local INPATH '/home/training/student_record.txt' 
     INTO TABLE Student;

import org.apache.hadoop.hive.ql.exec.UDF;

public class GetMaxMarks extends UDF{

	public double evaluate (double math,double eng,double physics,double chemis )
	{
		double maxMarks=math;
		if(eng>maxMarks)
		{
			maxMarks = eng;
		}
		if(physics>maxMarks)
		{
			maxMarks=physics;
		}
		if(chemis>maxMarks)
		{
			maxMarks=chemis;
		}		
		return maxMarks;
	}
}

hive > add jar /home/training/hiveudf.jar;

hive > create temporary function xyz as 'GetMaxMarks';

select  math,english,physis,chemistry from Student;

select name,xyz( math,english,physis,chemistry ) from Student;

=========================================================================================
								UDAF in Hive
=========================================================================================
import org.apache.hadoop.hive.ql.exec.UDAF;
import org.apache.hadoop.hive.ql.exec.UDAFEvaluator;
import org.apache.hadoop.hive.serde2.io.DoubleWritable;

public class GetMeanMarks extends UDAF{
	public static class GetIntMeanEvaluator implements UDAFEvaluator
	{
	
		PartialResult part; 
		
		public void init()
		{
			part = null;
		}
		
		public boolean iterate(DoubleWritable value)
		{
			if (value == null)
			{
				return true;
			}
			if(part == null)
			{
				part = new PartialResult();
			}
			part.result = part.result + value.get();
			part.count++;
			return true;
		}
		
		public PartialResult terminatePartial()
		{
			return part;
		}
		
		public boolean merge(PartialResult otherFile)
		{
			if(otherFile == null)
			{
				return true;
			}
			if(part == null)
			{
				part= new PartialResult();
			}
			part.result = part.result + otherFile.result;
			part.count = part.count + otherFile.count;
			return true;
		}
		
		public DoubleWritable terminate()
		{
			if ( part == null)
			{
				return null;
			}
			return new DoubleWritable( (part.result)/part.count);
		}

	}
}

public class PartialResult {
	double result;
	int count;
}

hive > add jar /home/training/hive-udf.jar;

hive > create temporary function getMeanMarks as 'GetMeanMarks';

select getMaxMarks( name ) from Student;
select getMeanMarks( math,english,physis,chemistry ) from Student;

=========================================================================================
								UDTF in Hive-
=========================================================================================
Create a file name book_details.txt

1001	38752984,Who Moved My Cheese ?,Spencer Johnson	Friction
1002	12121212,The Power of Positive Thinking,Norman Vincent	Philosophy


CREATE TABLE Book ( book_id int, book_details String, genres string )
    ROW FORMAT DELIMITED
    FIELDS TERMINATED BY ‘\t’ 
 
LOAD DATA INPATH '/home/training/book_details.txt' 
     INTO TABLE Book;

package UDTF;

import java.util.ArrayList;

import org.apache.hadoop.hive.ql.metadata.HiveException;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.PrimitiveCategory;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;

public class ExpandBookDetail extends GenericUDTF{

	private Object[] fwdObj = null;
	private PrimitiveObjectInspector bookDtlOI = null;
	
	public StructObjectInspector initialize(ObjectInspector[] arg)
	{
		ArrayList<String> fieldNames = new ArrayList<String>();
		ArrayList<ObjectInspector> fieldOIs = new ArrayList<ObjectInspector>();
		
		bookDtlOI = (PrimitiveObjectInspector) arg[0];
		
		fieldNames.add("ISBN");
		fieldOIs.add(PrimitiveObjectInspectorFactory.getPrimitiveJavaObjectInspector(
		PrimitiveCategory.INT));
		
		fieldNames.add("TITLE");
		fieldOIs.add(PrimitiveObjectInspectorFactory.getPrimitiveJavaObjectInspector(
		PrimitiveCategory.STRING));
		
		fieldNames.add("AUTHOR");
		fieldOIs.add(PrimitiveObjectInspectorFactory.getPrimitiveJavaObjectInspector(
		PrimitiveCategory.STRING));
		
		fwdObj = new Object[3];
		return ObjectInspectorFactory.getStandardStructObjectInspector(
				fieldNames, fieldOIs);
	}
	
	public void process(Object[] record) throws HiveException
	{
		String bookDtl = bookDtlOI.getPrimitiveJavaObject(record[0]).toString();
		
		String str[] = bookDtl.split(",");
		fwdObj[0] = Integer.parseInt(str[0]);
		fwdObj[1] = str[1];
		fwdObj[2] = str[2];
		
		this.forward(fwdObj);
		
	}
	
	public void close()
	{
		  
	}
}

hive > add jar /home/training/hive-udf.jar;

hive > create temporary function expandBookDetail as 'ExpandBookDetail';

select expandBookDetail( book_DETAILS ) as ( ISBN,Title,Author ) from Book;	
	 
=========================================================================================
					Import and Export using Sqoop from RDBMS to HIVE
========================================================================================
sqoop import \
   --connect "jdbc:mysql://localhost/training" \
  --username training \
  --password training \
  --table Movies \
  --hive-import \
  --hive-overwrite \
  --hive-table userdb.movies \
  --hive-home /user/hive/warehouse \
	-m 1
=========================================================================================
=========================================================================================
create table department_hive( id int, deparment_id string, avg_salary int );
 
create table department_hive01 ( id int(10),deparment_id varchar(50),avg_salary int(10));

insert into department_hive01 select d.*,null from department d;
 
insert into department_hive01 values ( 777, 'Not Known', 1000 );
insert into department_hive01 values ( 888, null, 1000 );
insert into department_hive01 values ( 666, null, 1100 );
 
sqoop import \
   --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --table department_hive01 \
  --hive-import \
  --hive-overwrite \
  --hive-table userdb.department_hive \
  --fields-terminated-by '\001' \
  --null-string "" \
  --null-non-string -999 \
  --split-by id \
  -m 1
 
sqoop export \
   --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --table department_hive02 \
  --export-dir /user/hive/warehouse/department_hive01
  --input-fields-terminated-by 
  --input-line-terminated-by 
  --num-mappers 1 \
  --batch \
  --input-null-string "" \
  --input-not-null-string -999

=========================================================================================
									Export
=========================================================================================

1.   Create contacthive.csv file which has simple data with 4 columns separated by comma

    1,MahendraSingh,Dhoni,mahendra@bcci.com
    2,Virat,Kohali,virat@bcci.com
    5,Sachin,Tendulkar,sachin@bcci.com

2.   Upload the contacthive.csv that you created in last step in HDFS at /tmp folder using following command

       hdfs dfs -put contacthive.csv contacthive.csv

3.   Define a contact_hive table that will have 4 columns, contactId, firstName, lastName and email, execute this command in hive console


    CREATE TABLE contact_hive(contactId Int, firstName String, lastName String, email String) row format delimited fields terminated by ',';

4.  In this step populate the contact_hive table that you created in the last step with the data from contacthive.csv file created in step 1. Execute this command in Hive console to populate contact_hive table

    LOAD DATA local INPATH  "/home/training/contacthive.csv" INTO TABLE contact_hive;

    Since i am using Hive managed table, it will move the contacthive.csv file to Hive managed directory in case of Hortonworks that directory is /apps/hive/warehouse, You can verify that by executing following command on HDFS


5.  hdfs dfs -ls /user/hive/warehouse/contact_hive

6.  Before you export data into RDBMS, you will have to create the table in mysql, use following command to create the Contact table in mysql.



    CREATE TABLE Contact (
          contactid INTEGER NOT NULL ,
          firstname VARCHAR(50),
          lastname  VARCHAR(50),
          email varchar(50)
    );

7.  Now last step is to execute sqoop export command that exports data from hive/hdfs directory to database


    sqoop export --connect jdbc:mysql://localhost/training --username training --password training --table Contact --export-dir /user/hive/warehouse/userdb.db/contact_hive/contacthive.csv
	
=========================================================================================
Create database if not exists Family
Comment 'This database will be used for collecting various family data and their daily habits'
location '/user'
with dbproperties ( 'Database creator' = 'Vedic', 'Database_Created_On'='2016-01-01')

show databases;

describe database extended family;

=========================================================================================
						How to load json data in Hive Table
=========================================================================================
Download from http://files.cloudera.com/samples/hiv...
copy this file into hive/lib path.

add jar /usr/lib/hive/lib/hive-serdes-1.0-SNAPSHOT.jar;

create external table artist ( id string, last_name string,first_name string,year_of_birth string ) ROW FORMAT SERDE 'com.cloudera.hive.serde.JSONSerDe' location '/user/training/artists_en.json';

=========================================================================================
						How to load xml data in Hive Table
=========================================================================================
cat employees.xml
<employee>
<id>1</id>
<name>Satish Kumar</name>
<designation>Technical Lead</designation>
</employee>
<employee>
<id>2</id>
<name>Ramya</name>
<designation>Testing</designation>
</employee>

cat employees.xml | tr -d '&' | tr '\n' ' ' | tr '\r' ' ' | sed 's|</employee>|</employee>\n|g' | grep -v '^\s*$' > employees_records.xml

cat employees_records.xml

<employee> <id>1</id> <name>Satish Kumar</name> <designation>Technical Lead</designation> </employee>
 <employee> <id>2</id> <name>Ramya</name> <designation>Testing</designation> </employee>


hadoop fs -mkdir /user/hive/sample-xml-inputs

hadoop fs -put employees_records.xml /user/hive/sample-xml-inputs

hadoop fs -cat /user/hive/sample-xml-inputs/employees_records.xml

create external table xml_table_org( xmldata string) LOCATION '/user/hive/sample-xml-inputs/';

select * from xml_table_org;

CREATE TABLE xml_table AS SELECT xpath_int(xmldata,'employee/id')as empid ,xpath_string(xmldata,'employee/name')as empname ,xpath_string(xmldata,'employee/designation') as desg FROM xml_table_org;

select * from xml_table; 
                                      OK
1 Satish Kumar Technical Lead
2 Ramya Testing


=========================================================================================
==================================================================================
cat > test_details.txt

sudo gedit test_details.txt
100,1001
105,1002
110,1003
115,1004
120,1005

CREATE TABLE test_details_txt( visit_id INT, store_id SMALLINT) 
row format delimited
fields terminated by ','
STORED AS TEXTFILE;

CREATE TABLE test_details_orc( visit_id INT, store_id SMALLINT) STORED AS ORC;

-- Load into Text table
LOAD DATA LOCAL INPATH '/home/cloudera/test_details.txt' INTO TABLE test_details_txt;

-- Copy to ORC table
INSERT INTO TABLE test_details_orc SELECT * FROM test_details_txt;

describe formatted table_name;


create table txnrecords_txt(txnno INT, txndate STRING, custno INT, amount DOUBLE, 
category STRING, product STRING, city STRING, state STRING, spendby STRING)
row format delimited
fields terminated by ','
stored as textfile;

LOAD DATA LOCAL INPATH '/home/cloudera/txns' OVERWRITE INTO TABLE txnrecords_txt;

create table txnrecords_orc(txnno INT, txndate STRING, custno INT, amount DOUBLE, 
category STRING, product STRING, city STRING, state STRING, spendby STRING)
STORED AS ORC;

-- Copy to ORC table
INSERT INTO TABLE txnrecords_orc SELECT * FROM txnrecords_txt;

=========================================================================================
							Impala
=========================================================================================
impala-shell
invalidate metadata

==================================================================================================
							     Hive Use Case - Airline Analysis
==================================================================================================

1,Goroka,Goroka,Papua New Guinea,GKA,AYGA,-6.081689,145.391881,5282,10,U,Pacific/Port_Moresby
2,Madang,Madang,Papua New Guinea,MAG,AYMD,-5.207083,145.7887,20,10,U,Pacific/Port_Moresby
3,Mount Hagen,Mount Hagen,Papua New Guinea,HGU,AYMH,-5.826789,144.295861,5388,10,U,Pacific/Port_Moresby
4,Nadzab,Nadzab,Papua New Guinea,LAE,AYNZ,-6.569828,146.726242,239,10,U,Pacific/Port_Moresby
5,Port Moresby Jacksons Intl,Port Moresby,Papua New Guinea,POM,AYPY,-9.443383,147.22005,146,10,U,Pacific/Port_Moresby
6,Wewak Intl,Wewak,Papua New Guinea,WWK,AYWK,-3.583828,143.669186,19,10,U,Pacific/Port_Moresby
7,Narsarsuaq,Narssarssuaq,Greenland,UAK,BGBW,61.160517,-45.425978,112,-3,E,America/Godthab
8,Nuuk,Godthaab,Greenland,GOH,BGGH,64.190922,-51.678064,283,-3,E,America/Godthab
9,Sondre Stromfjord,Sondrestrom,Greenland,SFJ,BGSF,67.016969,-50.689325,165,-3,E,America/Godthab
10,Thule Air Base,Thule,Greenland,THU,BGTL,76.531203,-68.703161,251,-4,E,America/Thule

CREATE TABLE Airport ( id int, name String, city String, country string, FAA string, ICAO string, latitude float, longitude float, altitude int, timezone float, dst string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';

load data local inpath '/home/training/airports_mod.dat'
into table Airport;

1,Private flight,\N,-,N/A,,,Y
2,135 Airways,\N,,GNL,GENERAL,United States,N
3,1Time Airline,\N,1T,RNX,NEXTIME,South Africa,Y
4,2 Sqn No 1 Elementary Flying Training School,\N,,WYT,,United Kingdom,N
5,213 Flight Unit,\N,,TFU,,Russia,N
6,223 Flight Unit State Airline,\N,,CHD,CHKALOVSK-AVIA,Russia,N
7,224th Flight Unit,\N,,TTF,CARGO UNIT,Russia,N
8,247 Jet Ltd,\N,,TWF,CLOUD RUNNER,United Kingdom,N
9,3D Aviation,\N,,SEC,SECUREX,United States,N
10,40-Mile Air,\N,Q5,MLA,MILE-AIR,United States,Y

CREATE TABLE Airlines ( airline_id int, name String, alias String, IATA string, ICAO string, country string, status string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ','; 

load data local inpath '/home/training/Final_airlines'
into table Airlines;

2B,410,AER,2965,KZN,2990,,0,CR2
2B,410,ASF,2966,KZN,2990,,0,CR2
2B,410,ASF,2966,MRV,2962,,0,CR2
2B,410,CEK,2968,KZN,2990,,0,CR2
2B,410,CEK,2968,OVB,4078,,0,CR2
2B,410,DME,4029,KZN,2990,,0,CR2
2B,410,DME,4029,NBC,6969,,0,CR2
2B,410,DME,4029,TGK,\N,,0,CR2
2B,410,DME,4029,UUA,6160,,0,CR2
2B,410,EGO,6156,KGD,2952,,0,CR2

CREATE TABLE Routes ( airline_code string, airline_id int, source String, source_IS int, destination string, dest_ID int, codeshare string, stops int, equipment string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ','; 

load data local inpath '/home/training/routes.dat'
into table Routes;

=========================================================================================
1. select id,name from airport where country = 'India';

2. select DISTINCT (airlines.name) from airlines join routes on ( airlines.airline_id = routes.airline_id ) WHERE routes.stops = 0;

3. select DISTINCT (airlines.name) from airlines join routes on ( airlines.airline_id = routes.airline_id ) WHERE routes.codeshare = 'Y';

4. SELECT altitude,country from airport ORDER BY altitude DESC LIMIT 1;

5. 

Creating Tables
========================================================================

CREATE TABLE Airport ( id int, name String, city String, country string, FAA string, ICAO string, latitude float, longitude float, altitude int, timezone float, dst string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';

load data local inpath '/home/training/airports_mod.dat'
into table Airport;

CREATE TABLE Airlines ( airline_id int, name String, alias String, IATA string, ICAO string, country string, status string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ','; 

load data local inpath '/home/training/Final_airlines'
into table Airlines;

CREATE TABLE Routes ( airline_code string, airline_id int, source String, source_IS int, destination string, dest_ID int, codeshare string, stops int, equipment string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ','; 

load data local inpath '/home/training/routes.dat'
into table Routes;


Loading DATA
=========================================================================
LOAD DATA local INPATH '/home/training/Assignments/routes.txt' 
INTO TABLE routes;

Insert overwrite directory '/user/training/Assignments/' select * from airport;
	 
Ans#1

Insert overwrite directory '/user/training/Assignments/' select id,name from airport where country = 'India';

Ans#2

Insert overwrite directory '/user/training/Assignments/' select DISTINCT (airlines.name) from airlines join routes on ( airlines.airline_id = routes.airline_id ) WHERE routes.stops = 0;

Ans#3

Insert overwrite directory '/user/training/Assignments/' select DISTINCT (airlines.name) from airlines join routes on ( airlines.airline_id = routes.airline_id ) WHERE routes.codeshare = 'Y';

Ans#4

Insert overwrite directory '/user/training/Assignments/' SELECT altitude,country from airport ORDER BY altitude DESC LIMIT 1;

Ans#5

create table activestatus ( airline_id int, name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';

insert overwrite table activestatus select airline_id,name from airlines where status = 'Y';

select airport.name from airport join activestatus on (airport.id=activestatus.airline_id) where airport.name = 'United States';


=========================================================================================
				        Hive Use Case -- Maximum Temperature with City	  
=========================================================================================
1950,Hydrabad,12
1950,Kolkata,13
1950,Delhi,15
1950,Bombay,17
1950,Chennai,16
1950,Bangalore,18
1950,Pune,20
1949,Hydrabad,12
1949,Kolkata,13
1949,Delhi,30
1949,Chennai,17
1949,Pune,16
1949,Bangalore,18
1949,Bombay,19
1948,Hydrabad,12
1948,Chennai,13
1948,Delhi,15
1948,Bombay,17
1948,Pune,16
1948,Bangalore,20
1948,Kolkata,21

create table Temperature ( year int, city string, temp int ) row format delimited fields terminated by ',';

load data local inpath '/home/cloudera/temp.txt' into table Temperature;

select * from Temperature;

create table MaxTemperature ( year int, maxtemp int ) row format delimited fields terminated by ',';

select * from MaxTemperature;

insert overwrite table MaxTemperature select year, max(temp) from Temperature group by year;

select * from MaxTemperature;

create table FinalTemperatureResult ( year int, city string,maxtemp int ) row format delimited fields terminated by ',';

insert overwrite table FinalTemperatureResult select a.year,a.city, b.maxtemp from Temperature a JOIN MaxTemperature b ON a.temp = b.maxtemp and a.year=b.year order by a.year;

select * from FinalTemperatureResult;

=========================================================================================
				        Hive Use Case -- Country Project	  
=========================================================================================
CREATE TABLE Countries ( name string, landmass int, zone int,area int,population int,language int,religion int,bar int,stripes int,colours int,red int,green int,blue int,gold int,white int,black int,orange int,mainhue string,circles int,crosses int,saltires int,quarters int,sunstars int,crescent int,triangle int,icon int,animate int,text int,topleft string,botright string ) row format delimited fields terminated by ',';

load data local inpath '/home/training/countrydataset.txt'
into table Countries;

select * from Countries;

select count(*) from Countries;


Country Project

Problem Statement

A.	Count number of countries based on landmass.

select landmass , count(name) from Countries group by landmass;

B.	Find out top 5 country with Sum of bars and strips in a flag.

create table SumBarsStrips ( name string, sumofbarandstrips int ) row format delimited fields terminated by ',';

insert overwrite table SumBarsStrips select name, bar+stripes from Countries;

select name , sumofbarandstrips from SumBarsStrips order by sumofbarandstrips desc limit 5;

C.	Count of countries with icon.

select icon, count(name) from Countries group by icon having icon=1;

D.	Count of countries which have same top left and top right color in flag.

select count(name) from Countries where topleft = botright;

E.	Count number of countries based on zone.

select count(name,zone from countries

F.	Find out largest county in terms of area in NE zone.

G.	Find out least populated country in S.America landmass.

H.	Find out largest speaking language among all countries.

I.	Find most common colour among flags from all countries.

J.	Sum of all circles present in all country flags.

K.	Count of countries which have both icon and text in flag.


=========================================================================================
				        Hive Use Case -- Youtube Data Analysis	  
=========================================================================================
Creation of table
==========================================================================
CREATE TABLE Youtube ( videoid string, uploaderofvideo string, ageofvideo int, categoryofvideo string, lengthofvideo int, numberofviews int, ratingofvideo float, ratingcount int, numberofcomments int, relatedvideoids int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';

Loading Data
==========================================================================

load data local inpath '/home/training/youtubedata.txt' into table Youtube;

select * from Youtube;

select count(*) from Youtube;

===========================================================================
create table CategoryWithNoOfVideos ( categoryofvideo string, countOfVideo int ) row format delimited fields terminated by '\t';

insert overwrite table CategoryWithNoOfVideos select categoryofvideo, count(videoid) from Youtube group by categoryofvideo;

select * from CategoryWithNoOfVideos;

insert overwrite directory '/user/training/youtube' select categoryofvideo , countOfVideo from CategoryWithNoOfVideos order by countOfVideo desc limit 5;

============================================================================

insert overwrite directory '/user/training/top10ratedvideos' select videoid , ratingofvideo from Youtube order by ratingofvideo desc limit 10;

============================================================================

insert overwrite directory '/user/training/mostviewsvideos' select videoid , numberofviews from Youtube order by numberofviews desc limit 1;

=========================================================================================
				        Hive Use Case -- Movie Data Analysis	  
=========================================================================================
create table Movies ( MovieId int,MovieName String,YearOfRelease int,Rating float,Duration bigint )  row format delimited fields terminated by ',';

load data local inpath '/home/training/moviedata.txt'
into table Movies

select MovieName from Movies where YearOfRelease > 1950 and YearOfRelease < 1960;

select count(MovieName) from Movies where YearOfRelease > 1950 and YearOfRelease < 1960;

select count(MovieName) from Movies where Rating > 4;

select count(MovieName) from Movies where Rating > 3 and Rating < 4;

select count(MovieName) from Movies where Duration > 7200;

select YearOfRelease, count(MovieName) from Movies group by YearOfRelease;

select count(MovieName) from Movies;


=========================================================================================
				        Hive Use Case -- Stack Overflow   
=========================================================================================
create table Comments ( commentid bigint, userid bigint ) row format delimited
fields terminated by ',';

load data local inpath '/home/training/comments.csv' into table Comments;

create table Posts ( postid int, post_type int,creationdate string,score int,viewcount int,owneruserid int,title string,answercount int, commentcount int) row format delimited
fields terminated by ',';

load data local inpath '/home/training/posts.csv' into table Posts;

create table PostTypes ( postid int, postname string ) row format delimited
fields terminated by ',';

load data local inpath '/home/training/posttypes.csv' into table PostTypes;

create table Users ( userid bigint, reputation bigint,displayname string,loc sting,age int) row format delimited fields terminated by ',';

load data local inpath '/home/training/users.csv' into table Users;


create table UsersReputation ( userid bigint, displayname string ,reputation bigint ) row format delimited fields terminated by ',';

insert overwrite table UsersReputation select userid, displayname,reputation from Users order by reputation desc limit 1;

select  a.displayname , count( b.postid ) from UsersReputation a JOIN Posts b ON a.userid = b.owneruserid;


select avg(age) from Users;


select owneruserid,creationdate from Posts order by creationdate limit 1;

select displayname,reputation from Users order by reputation desc limit 1;

=================================================================================================
